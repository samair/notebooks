{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-08T14:28:57.247981Z",
     "start_time": "2025-08-08T14:28:56.430222Z"
    }
   },
   "source": [
    "from mlx_lm import generate, load\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sameerchandra/PycharmProjects/multi-modal-researcher/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T14:29:26.100502Z",
     "start_time": "2025-08-08T14:29:23.972659Z"
    }
   },
   "cell_type": "code",
   "source": "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.3-4bit\")\n",
   "id": "277a34dea3cdb275",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 104484.44it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:08:49.033400Z",
     "start_time": "2025-08-09T06:08:47.767599Z"
    }
   },
   "cell_type": "code",
   "source": "model, tokenizer = load(\"mlx-community/Qwen3-1.7B-4bit\")",
   "id": "ff89909aa9abfbf3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 76260.07it/s]\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:17:33.176273Z",
     "start_time": "2025-08-09T06:17:29.956516Z"
    }
   },
   "cell_type": "code",
   "source": "model, tokenizer = load(\"mlx-community/gemma-3-4b-it-4bit\")",
   "id": "6bbfc404294c6ad8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 76491.87it/s]\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:17:36.309706Z",
     "start_time": "2025-08-09T06:17:36.307079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)"
   ],
   "id": "d1e42416a1d7ec5c",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:17:37.864670Z",
     "start_time": "2025-08-09T06:17:37.857354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# User turn\n",
    "prompt = \"2+2-2*6+2\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)"
   ],
   "id": "41acd85890b08851",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:17:43.842110Z",
     "start_time": "2025-08-09T06:17:39.827546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_tokens=2000,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n"
   ],
   "id": "8f3b069cfc8d062b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Let's solve the expression using the order of operations (PEMDAS/BODMAS):\n",
      "\n",
      "1. **Multiplication:** 2 * 6 = 12\n",
      "   The expression becomes: 2 + 2 - 12 + 2\n",
      "\n",
      "2. **Addition and Subtraction** (from left to right):\n",
      "   2 + 2 = 4\n",
      "   4 - 12 = -8\n",
      "   -8 + 2 = -6\n",
      "\n",
      "Therefore, 2 + 2 - 2 * 6 + 2 = -6\n",
      "\n",
      "**Final Answer: -6**\n",
      "==========\n",
      "Prompt: 18 tokens, 32.644 tokens-per-sec\n",
      "Generation: 126 tokens, 36.576 tokens-per-sec\n",
      "Peak memory: 5.839 GB\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "97bc8d94b3f0ffeb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
